{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db8cb0a6-458c-49da-a936-61213b3f469e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TimeoutException",
     "evalue": "Message: \n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 144\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… ì „ì²´ ë¦¬ë·° \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mê°œ ì €ì¥ â†’ all_reviews.csv, all_reviews.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 144\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 125\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    123\u001b[0m     wait\u001b[38;5;241m.\u001b[39muntil(EC\u001b[38;5;241m.\u001b[39melement_to_be_clickable((By\u001b[38;5;241m.\u001b[39mXPATH, selector)))\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 125\u001b[0m     \u001b[43mwait\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melement_to_be_clickable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCSS_SELECTOR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m    126\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# ë¦¬ë·° í¬ë¡¤ë§\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\TF210Py310\\lib\\site-packages\\selenium\\webdriver\\support\\wait.py:146\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll)\n\u001b[1;32m--> 146\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[1;31mTimeoutException\u001b[0m: Message: \n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ìµœì´ˆ 1íšŒë§Œ)\n",
    "# pip install selenium webdriver-manager pandas openpyxl\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "def go_to_next_page(driver, wait):\n",
    "    try:\n",
    "        curr = int(driver.find_element(By.CSS_SELECTOR, \"div.pageing > strong\").text.strip())\n",
    "    except:\n",
    "        return False\n",
    "    nxt = curr + 1\n",
    "    try:\n",
    "        wait.until(EC.element_to_be_clickable(\n",
    "            (By.CSS_SELECTOR, f'div.pageing > a[data-page-no=\"{nxt}\"]')\n",
    "        )).click()\n",
    "    except TimeoutException:\n",
    "        try:\n",
    "            wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"div.pageing > a.next\"))).click()\n",
    "            wait.until(EC.element_to_be_clickable(\n",
    "                (By.CSS_SELECTOR, f'div.pageing > a[data-page-no=\"{nxt}\"]')\n",
    "            )).click()\n",
    "        except TimeoutException:\n",
    "            return False\n",
    "    try:\n",
    "        wait.until(lambda d: d.find_element(By.CSS_SELECTOR, \"div.pageing > strong\").text.strip() == str(nxt))\n",
    "        return True\n",
    "    except TimeoutException:\n",
    "        return False\n",
    "\n",
    "\n",
    "def collect_reviews(driver, wait):\n",
    "    reviews = []\n",
    "    try:\n",
    "        wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"#reviewInfo > a\"))).click()\n",
    "        time.sleep(1)\n",
    "    except TimeoutException:\n",
    "        return reviews\n",
    "\n",
    "    while True:\n",
    "        wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"#gdasList > li\")))\n",
    "        for el in driver.find_elements(By.CSS_SELECTOR, \"#gdasList > li > div.review_cont > div.txt_inner\"):\n",
    "            txt = el.text.strip()\n",
    "            if txt:\n",
    "                reviews.append(txt)\n",
    "        if not go_to_next_page(driver, wait):\n",
    "            break\n",
    "    return reviews\n",
    "\n",
    "\n",
    "def crawl_reviews(driver, wait):\n",
    "    \"\"\"\n",
    "    í˜„ì¬ íƒ­ì—ì„œ ìƒí’ˆ ë§í¬ ìˆ˜ì§‘â†’ìƒˆ íƒ­ ì—´ê¸°â†’ë¸Œëœë“œ/ìƒí’ˆëª…/ë¦¬ë·° ìˆ˜ì§‘ í›„ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\n",
    "    ë°˜í™˜ í˜•ì‹: [[brand, product, review], ...]\n",
    "    \"\"\"\n",
    "    main_handle = driver.current_window_handle\n",
    "    links = [a.get_attribute(\"href\")\n",
    "             for a in driver.find_elements(By.CSS_SELECTOR, \"ul.cate_prd_list > li .prd_info a\")]\n",
    "\n",
    "    rows = []\n",
    "    for href in links:\n",
    "        driver.execute_script(\"window.open(arguments[0], '_blank');\", href)\n",
    "        driver.switch_to.window(driver.window_handles[-1])\n",
    "        time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            brand = driver.find_element(By.CSS_SELECTOR, \"#moveBrandShop\").text.strip()\n",
    "        except:\n",
    "            brand = \"\"\n",
    "        try:\n",
    "            product = driver.find_element(By.CSS_SELECTOR, \"p.prd_name\").text.strip()\n",
    "        except:\n",
    "            product = \"\"\n",
    "\n",
    "        reviews = collect_reviews(driver, wait)\n",
    "        for rev in reviews:\n",
    "            rows.append([brand, product, rev])\n",
    "\n",
    "        driver.close()\n",
    "        # ì•ˆì „í•˜ê²Œ ì›ë˜ íƒ­ìœ¼ë¡œ ë³µê·€\n",
    "        handles = driver.window_handles\n",
    "        if main_handle in handles:\n",
    "            driver.switch_to.window(main_handle)\n",
    "        elif handles:\n",
    "            driver.switch_to.window(handles[0])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return rows\n",
    "\n",
    "def main():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_experimental_option(\"detach\", True)\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    driver.get(\"https://www.oliveyoung.co.kr/store/main/main.do\")\n",
    "    wait.until(EC.element_to_be_clickable((By.ID, \"btnGnbOpen\"))).click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    all_rows = []\n",
    "    categories = [\n",
    "        (\"ìŠ¤í‚¨/í† ë„ˆ\",      \"//a[contains(text(), 'ìŠ¤í‚¨/í† ë„ˆ')]\"),\n",
    "        (\"ì—ì„¼ìŠ¤/ì„¸ëŸ¼/ì•°í”Œ\", \"a[data-attr*='ì—ì„¼ìŠ¤/ì„¸ëŸ¼/ì•°í”Œ']\"),\n",
    "        (\"í¬ë¦¼\",           \"//ul[@class='loc_history']//a[@class='cate_y' and normalize-space(text())='í¬ë¦¼']\"),\n",
    "        (\"ì•„ì´í¬ë¦¼\",       \"//ul[@class='cate_list_box']//a[normalize-space(text())='ì•„ì´í¬ë¦¼']\")\n",
    "    ]\n",
    "\n",
    "    for cat_name, selector in categories:\n",
    "        # ì¹´í…Œê³ ë¦¬ í´ë¦­\n",
    "        if selector.startswith(\"//\"):\n",
    "            wait.until(EC.element_to_be_clickable((By.XPATH, selector))).click()\n",
    "        else:\n",
    "            wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, selector))).click()\n",
    "        time.sleep(1)\n",
    "\n",
    "        # ë¦¬ë·° í¬ë¡¤ë§\n",
    "        rows = crawl_reviews(driver, wait)\n",
    "        # ì¹´í…Œê³ ë¦¬ëª… ì»¬ëŸ¼ ì¶”ê°€\n",
    "        for r in rows:\n",
    "            all_rows.append([cat_name] + r)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ì €ì¥\n",
    "    df = pd.DataFrame(all_rows, columns=[\"ì¹´í…Œê³ ë¦¬\", \"ë¸Œëœë“œëª…\", \"ìƒí’ˆëª…\", \"ë¦¬ë·°ë‚´ìš©\"])\n",
    "    df.to_csv(\"all_reviews.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    df.to_excel(\"all_reviews.xlsx\", index=False)\n",
    "    print(f\"âœ… ì „ì²´ ë¦¬ë·° {len(df)}ê°œ ì €ì¥ â†’ all_reviews.csv, all_reviews.xlsx\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "629f1fe5-3b7e-4cf8-b8f7-1750501e66f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â–¶â–¶ [ìŠ¤í‚¨/í† ë„ˆ] í¬ë¡¤ë§ ì‹œì‘\n",
      "âš ï¸ [ìŠ¤í‚¨/í† ë„ˆ] í´ë¦­ ì‹¤íŒ¨, ê±´ë„ˆëœë‹ˆë‹¤.\n",
      "\n",
      "â–¶â–¶ [ì—ì„¼ìŠ¤/ì„¸ëŸ¼/ì•°í”Œ] í¬ë¡¤ë§ ì‹œì‘\n",
      "âš ï¸ [ì—ì„¼ìŠ¤/ì„¸ëŸ¼/ì•°í”Œ] í´ë¦­ ì‹¤íŒ¨, ê±´ë„ˆëœë‹ˆë‹¤.\n",
      "\n",
      "â–¶â–¶ [í¬ë¦¼] í¬ë¡¤ë§ ì‹œì‘\n",
      "âš ï¸ [í¬ë¦¼] í´ë¦­ ì‹¤íŒ¨, ê±´ë„ˆëœë‹ˆë‹¤.\n",
      "\n",
      "â–¶â–¶ [ì•„ì´í¬ë¦¼] í¬ë¡¤ë§ ì‹œì‘\n",
      "âš ï¸ [ì•„ì´í¬ë¦¼] í´ë¦­ ì‹¤íŒ¨, ê±´ë„ˆëœë‹ˆë‹¤.\n",
      "\n",
      "ğŸ‰ ëª¨ë“  ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ìµœì´ˆ 1íšŒë§Œ)\n",
    "# pip install selenium webdriver-manager pandas openpyxl\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "BASE_URL = \"https://www.oliveyoung.co.kr/store/main/main.do\"\n",
    "\n",
    "# ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ (í˜ì´ì§• ì²˜ë¦¬)\n",
    "def go_to_next_page(driver, wait):\n",
    "    try:\n",
    "        curr = int(driver.find_element(By.CSS_SELECTOR, \"div.pageing > strong\").text.strip())\n",
    "    except:\n",
    "        return False\n",
    "    nxt = curr + 1\n",
    "    try:\n",
    "        wait.until(EC.element_to_be_clickable(\n",
    "            (By.CSS_SELECTOR, f'div.pageing > a[data-page-no=\"{nxt}\"]')\n",
    "        )).click()\n",
    "    except TimeoutException:\n",
    "        # 'ë‹¤ìŒ 10í˜ì´ì§€' ë²„íŠ¼ í´ë¦­ í›„ ì¬ì‹œë„\n",
    "        try:\n",
    "            wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"div.pageing > a.next\"))).click()\n",
    "            wait.until(EC.element_to_be_clickable(\n",
    "                (By.CSS_SELECTOR, f'div.pageing > a[data-page-no=\"{nxt}\"]')\n",
    "            )).click()\n",
    "        except TimeoutException:\n",
    "            return False\n",
    "    try:\n",
    "        wait.until(lambda d: d.find_element(By.CSS_SELECTOR, \"div.pageing > strong\").text.strip() == str(nxt))\n",
    "        return True\n",
    "    except TimeoutException:\n",
    "        return False\n",
    "\n",
    "# í•œ ìƒí’ˆ í˜ì´ì§€ì—ì„œ ë¦¬ë·° ëª¨ë‘ ìˆ˜ì§‘\n",
    "def collect_reviews(driver, wait):\n",
    "    reviews = []\n",
    "    try:\n",
    "        wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"#reviewInfo > a\"))).click()\n",
    "        time.sleep(1)\n",
    "    except TimeoutException:\n",
    "        return reviews\n",
    "\n",
    "    while True:\n",
    "        wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"#gdasList > li\")))\n",
    "        for el in driver.find_elements(By.CSS_SELECTOR, \"#gdasList > li > div.review_cont > div.txt_inner\"):\n",
    "            txt = el.text.strip()\n",
    "            if txt:\n",
    "                reviews.append(txt)\n",
    "        if not go_to_next_page(driver, wait):\n",
    "            break\n",
    "    return reviews\n",
    "\n",
    "# í˜„ì¬ ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ì—ì„œ ëª¨ë“  ìƒí’ˆ ë§í¬ ì—´ê³  ë¦¬ë·° ìˆ˜ì§‘\n",
    "def crawl_reviews(driver, wait):\n",
    "    main_handle = driver.current_window_handle\n",
    "    links = [a.get_attribute(\"href\") for a in driver.find_elements(By.CSS_SELECTOR, \"ul.cate_prd_list > li .prd_info a\")]\n",
    "    rows = []\n",
    "\n",
    "    for href in links:\n",
    "        # ìƒˆ íƒ­ ì—´ê¸°\n",
    "        driver.execute_script(\"window.open(arguments[0], '_blank');\", href)\n",
    "        driver.switch_to.window(driver.window_handles[-1])\n",
    "        time.sleep(1)\n",
    "\n",
    "        # ë¸Œëœë“œëª…/ìƒí’ˆëª…\n",
    "        try:\n",
    "            brand = driver.find_element(By.CSS_SELECTOR, \"#moveBrandShop\").text.strip()\n",
    "        except:\n",
    "            brand = \"\"\n",
    "        try:\n",
    "            product = driver.find_element(By.CSS_SELECTOR, \"p.prd_name\").text.strip()\n",
    "        except:\n",
    "            product = \"\"\n",
    "\n",
    "        # ë¦¬ë·° ìˆ˜ì§‘\n",
    "        for rev in collect_reviews(driver, wait):\n",
    "            rows.append([brand, product, rev])\n",
    "\n",
    "        # íƒ­ ë‹«ê³  ì›ë˜ íƒ­ìœ¼ë¡œ ë³µê·€\n",
    "        driver.close()\n",
    "        handles = driver.window_handles\n",
    "        if main_handle in handles:\n",
    "            driver.switch_to.window(main_handle)\n",
    "        else:\n",
    "            driver.switch_to.window(handles[0])\n",
    "\n",
    "    return rows\n",
    "\n",
    "# GNB ë©”ë‰´ ì—´ê³  data-attr ì…€ë ‰í„°ë¡œ ì¹´í…Œê³ ë¦¬ í´ë¦­\n",
    "def click_category(driver, wait, css_selector):\n",
    "    driver.get(BASE_URL)\n",
    "    wait.until(EC.element_to_be_clickable((By.ID, \"btnGnbOpen\"))).click()\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        el = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, css_selector)))\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", el)\n",
    "        el.click()\n",
    "        time.sleep(2)\n",
    "        return True\n",
    "    except TimeoutException:\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_experimental_option(\"detach\", True)\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    wait = WebDriverWait(driver, 15)\n",
    "\n",
    "    # ì¹´í…Œê³ ë¦¬ëª… â†’ data-attr ê¸°ë°˜ CSS ì…€ë ‰í„° ë§¤í•‘\n",
    "    category_selectors = {\n",
    "        \"ìŠ¤í‚¨/í† ë„ˆ\":      \"a[data-attr*='ì¹´í…Œê³ ë¦¬ìƒì„¸^ì¹´í…Œê³ ë¦¬ë¦¬ìŠ¤íŠ¸^ìŠ¤í‚¨/í† ë„ˆ']\",\n",
    "        \"ì—ì„¼ìŠ¤/ì„¸ëŸ¼/ì•°í”Œ\": \"a[data-attr*='ì¹´í…Œê³ ë¦¬ìƒì„¸^ì¹´í…Œê³ ë¦¬ë¦¬ìŠ¤íŠ¸^ì—ì„¼ìŠ¤/ì„¸ëŸ¼/ì•°í”Œ']\",\n",
    "        \"í¬ë¦¼\":           \"a[data-attr*='ì¹´í…Œê³ ë¦¬ìƒì„¸^ì¹´í…Œê³ ë¦¬ë¦¬ìŠ¤íŠ¸^í¬ë¦¼']\",\n",
    "        \"ì•„ì´í¬ë¦¼\":       \"a[data-attr*='ì¹´í…Œê³ ë¦¬ìƒì„¸^ì¹´í…Œê³ ë¦¬ë¦¬ìŠ¤íŠ¸^ì•„ì´í¬ë¦¼']\"\n",
    "    }\n",
    "\n",
    "    for cat_name, selector in category_selectors.items():\n",
    "        print(f\"\\nâ–¶â–¶ [{cat_name}] í¬ë¡¤ë§ ì‹œì‘\")\n",
    "        if not click_category(driver, wait, selector):\n",
    "            print(f\"âš ï¸ [{cat_name}] í´ë¦­ ì‹¤íŒ¨, ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "            continue\n",
    "\n",
    "        rows = crawl_reviews(driver, wait)\n",
    "        if not rows:\n",
    "            print(f\"âš ï¸ [{cat_name}] ìˆ˜ì§‘ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            continue\n",
    "\n",
    "        # DataFrame ìƒì„± & íŒŒì¼ëª… ì•ˆì „ ì²˜ë¦¬\n",
    "        df = pd.DataFrame(rows, columns=[\"ë¸Œëœë“œëª…\", \"ìƒí’ˆëª…\", \"ë¦¬ë·°ë‚´ìš©\"])\n",
    "        safe_name = cat_name.replace(\"/\", \"_\")\n",
    "        csv_name = f\"{safe_name}_reviews.csv\"\n",
    "        xlsx_name = f\"{safe_name}_reviews.xlsx\"\n",
    "\n",
    "        df.to_csv(csv_name, index=False, encoding=\"utf-8-sig\")\n",
    "        df.to_excel(xlsx_name, index=False)\n",
    "        print(f\"âœ… [{cat_name}] ë¦¬ë·° {len(df)}ê°œ ì €ì¥ â†’ {csv_name}, {xlsx_name}\")\n",
    "\n",
    "    driver.quit()\n",
    "    print(\"\\nğŸ‰ ëª¨ë“  ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì™„ë£Œ!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f245db0-45ec-40a0-bcce-bb067869e2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â–¶â–¶ [ìŠ¤í‚¨/í† ë„ˆ] í¬ë¡¤ë§ ì‹œì‘\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ìµœì´ˆ 1íšŒë§Œ)\n",
    "# pip install selenium webdriver-manager pandas openpyxl\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "BASE_URL = \"https://www.oliveyoung.co.kr/store/main/main.do\"\n",
    "\n",
    "def go_to_next_page(driver, wait):\n",
    "    try:\n",
    "        curr = int(driver.find_element(By.CSS_SELECTOR, \"div.pageing > strong\").text.strip())\n",
    "    except:\n",
    "        return False\n",
    "    nxt = curr + 1\n",
    "    try:\n",
    "        wait.until(EC.element_to_be_clickable(\n",
    "            (By.CSS_SELECTOR, f\"div.pageing > a[data-page-no='{nxt}']\"))).click()\n",
    "    except TimeoutException:\n",
    "        try:\n",
    "            wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"div.pageing > a.next\"))).click()\n",
    "            wait.until(EC.element_to_be_clickable(\n",
    "                (By.CSS_SELECTOR, f\"div.pageing > a[data-page-no='{nxt}']\"))).click()\n",
    "        except TimeoutException:\n",
    "            return False\n",
    "    try:\n",
    "        wait.until(lambda d: d.find_element(By.CSS_SELECTOR, \"div.pageing > strong\").text.strip() == str(nxt))\n",
    "        return True\n",
    "    except TimeoutException:\n",
    "        return False\n",
    "\n",
    "def collect_reviews(driver, wait):\n",
    "    reviews = []\n",
    "    try:\n",
    "        wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"#reviewInfo > a\"))).click()\n",
    "        time.sleep(1)\n",
    "    except TimeoutException:\n",
    "        return reviews\n",
    "\n",
    "    while True:\n",
    "        wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"#gdasList > li\")))\n",
    "        for el in driver.find_elements(By.CSS_SELECTOR, \"#gdasList > li > div.review_cont > div.txt_inner\"):\n",
    "            txt = el.text.strip()\n",
    "            if txt:\n",
    "                reviews.append(txt)\n",
    "        if not go_to_next_page(driver, wait):\n",
    "            break\n",
    "    return reviews\n",
    "\n",
    "def crawl_reviews(driver, wait):\n",
    "    main_handle = driver.current_window_handle\n",
    "    links = [a.get_attribute(\"href\")\n",
    "             for a in driver.find_elements(By.CSS_SELECTOR, \"ul.cate_prd_list > li .prd_info a\")]\n",
    "    rows = []\n",
    "\n",
    "    for href in links:\n",
    "        driver.execute_script(\"window.open(arguments[0], '_blank')\", href)\n",
    "        driver.switch_to.window(driver.window_handles[-1])\n",
    "        time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            brand = driver.find_element(By.CSS_SELECTOR, \"#moveBrandShop\").text.strip()\n",
    "        except:\n",
    "            brand = \"\"\n",
    "        try:\n",
    "            product = driver.find_element(By.CSS_SELECTOR, \"p.prd_name\").text.strip()\n",
    "        except:\n",
    "            product = \"\"\n",
    "\n",
    "        for rev in collect_reviews(driver, wait):\n",
    "            rows.append([brand, product, rev])\n",
    "\n",
    "        driver.close()\n",
    "        handles = driver.window_handles\n",
    "        if main_handle in handles:\n",
    "            driver.switch_to.window(main_handle)\n",
    "        else:\n",
    "            driver.switch_to.window(handles[0])\n",
    "\n",
    "    return rows\n",
    "\n",
    "def click_category(driver, wait, xpath):\n",
    "    wait.until(EC.element_to_be_clickable((By.ID, \"btnGnbOpen\"))).click()\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        el = wait.until(EC.element_to_be_clickable((By.XPATH, xpath)))\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", el)\n",
    "        el.click()\n",
    "        time.sleep(2)\n",
    "        return True\n",
    "    except TimeoutException:\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_experimental_option(\"detach\", True)\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    wait = WebDriverWait(driver, 15)\n",
    "\n",
    "    # ì¹´í…Œê³ ë¦¬ XPaths\n",
    "    XPATHS = {\n",
    "        \"ìŠ¤í‚¨/í† ë„ˆ\":      \"//a[text()='ìŠ¤í‚¨/í† ë„ˆ']\",\n",
    "        \"ì—ì„¼ìŠ¤/ì„¸ëŸ¼/ì•°í”Œ\": \"//a[text()='ì—ì„¼ìŠ¤/ì„¸ëŸ¼/ì•°í”Œ']\",\n",
    "        \"í¬ë¦¼\":           \"//a[text()='í¬ë¦¼']\",\n",
    "        \"ì•„ì´í¬ë¦¼\":       \"//a[text()='ì•„ì´í¬ë¦¼']\",\n",
    "    }\n",
    "\n",
    "    for cat in [\"ìŠ¤í‚¨/í† ë„ˆ\", \"ì—ì„¼ìŠ¤/ì„¸ëŸ¼/ì•°í”Œ\", \"í¬ë¦¼\", \"ì•„ì´í¬ë¦¼\"]:\n",
    "        print(f\"\\nâ–¶â–¶ [{cat}] í¬ë¡¤ë§ ì‹œì‘\")\n",
    "        driver.get(BASE_URL)\n",
    "\n",
    "        # GNB ì—´ê³  í´ë¦­\n",
    "        if cat != \"ì•„ì´í¬ë¦¼\":\n",
    "            if not click_category(driver, wait, XPATHS[cat]):\n",
    "                print(f\"âš ï¸ [{cat}] í´ë¦­ ì‹¤íŒ¨, ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "                continue\n",
    "        else:\n",
    "            # ì•„ì´í¬ë¦¼ì€ ë¨¼ì € í¬ë¦¼ í´ë¦­ â†’ ì•„ì´í¬ë¦¼ í´ë¦­\n",
    "            if not click_category(driver, wait, XPATHS[\"í¬ë¦¼\"]):\n",
    "                print(\"âš ï¸ [í¬ë¦¼] í´ë¦­ ì‹¤íŒ¨, ì•„ì´í¬ë¦¼ìœ¼ë¡œ ë„˜ì–´ê°ˆ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                continue\n",
    "            # í¬ë¦¼ í˜ì´ì§€ ë‚´ì—ì„œ ì•„ì´í¬ë¦¼ í´ë¦­\n",
    "            try:\n",
    "                el = wait.until(EC.element_to_be_clickable((By.XPATH, XPATHS[\"ì•„ì´í¬ë¦¼\"])))\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView(true);\", el)\n",
    "                el.click()\n",
    "                time.sleep(2)\n",
    "            except TimeoutException:\n",
    "                print(\"âš ï¸ [ì•„ì´í¬ë¦¼] í´ë¦­ ì‹¤íŒ¨\")\n",
    "                continue\n",
    "\n",
    "        rows = crawl_reviews(driver, wait)\n",
    "        if not rows:\n",
    "            print(f\"âš ï¸ [{cat}] ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            continue\n",
    "\n",
    "        df = pd.DataFrame(rows, columns=[\"ë¸Œëœë“œëª…\", \"ìƒí’ˆëª…\", \"ë¦¬ë·°ë‚´ìš©\"])\n",
    "        safe = cat.replace(\"/\", \"_\")\n",
    "        csv_name = f\"{safe}_reviews.csv\"\n",
    "        xlsx_name = f\"{safe}_reviews.xlsx\"\n",
    "        df.to_csv(csv_name, index=False, encoding=\"utf-8-sig\")\n",
    "        df.to_excel(xlsx_name, index=False)\n",
    "        print(f\"âœ… [{cat}] ë¦¬ë·° {len(df)}ê°œ ì €ì¥ â†’ {csv_name}, {xlsx_name}\")\n",
    "\n",
    "    driver.quit()\n",
    "    print(\"\\nğŸ‰ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TF210Py310)",
   "language": "python",
   "name": "tf210py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
